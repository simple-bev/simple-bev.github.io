<!DOCTYPE html>
<html>
  <head>
    <title>Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <!-- <link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Roboto'> -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <style>
      html,body,h1,h2,h3,h4,h5,h6 {font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;}
      <!-- .cite { background:#f0f0f0; padding:10px; font-size:18px} -->
      .cite { padding:0px; background:#ffffff; font-size:18px}
      .card {border: 1px solid #ccc}
      img { margin-bottom:-6px;}
      h1 { font-size:52px; font-weight:lighter}
      p { font-size:18px;}
      a {text-decoration: none; color: #2196F3;}
      .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
      0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
      5px 5px 0 0px #fff, /* The second layer */
      5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
      10px 10px 0 0px #fff, /* The third layer */
      10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
      15px 15px 0 0px #fff, /* The fourth layer */
      15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
      20px 20px 0 0px #fff, /* The fifth layer */
      20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
      25px 25px 0 0px #fff, /* The fifth layer */
      25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
      margin-left: 10px;
      margin-right: 60px;
      }
    </style>
  </head>  
  <body class="w3-white">
    <!-- Page Container -->
    <div class="w3-content w3-margin-top w3-margin-bottom" style="max-width:960px;">


      <!-- The Grid -->
      <div class="w3-row-padding">

	<!-- paper container -->	  
	<div class="w3-display-container w3-row w3-white w3-margin-bottom">
	  <div class="w3-center">
	    <h1>Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?</h1>
	    <h5><a href="https://adamharley.com/">Adam W. Harley</a> &emsp;&emsp; <a href="https://zfang399.github.io/">Zhaoyuan Fang</a> &emsp;&emsp; <a href="https://www.tri.global/about-us/jie-li/">Jie Li</a> &emsp;&emsp; <a href="https://www.csc.kth.se/~raambrus/">Rares Ambrus</a> &emsp;&emsp; <a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a></h5>
	    <h5><em>ICRA 2023</em></h5>
	  </div>
	  <div class="w3-center">
	    <h3>[<a href="https://github.com/aharley/simple_bev">Code</a>] &emsp; [<a href="https://arxiv.org/abs/2206.07959">Paper</a>]</h3>
	    <!-- <h3>[<a href="https://github.com/aharley/simple_bev">Code</a>] &emsp; [<a href="simple_bev_sep30.pdf">Paper</a>] &emsp; [<a href="https://arxiv.org/abs/2206.07959">arXiv</a>]</h3> -->
	    <!-- <h3>[<a href="https://github.com/aharley/simple_bev">Code</a>]</h3> -->
	  </div>
	  <hr>

	  
	  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
	    <h4>5 minute overview (click to unmute)</h4>
	    <!-- <img src="images/bev_vis.png" style="width:100%"> -->
	    <video controls style="width:100%;max-width:100%" autoplay="true" loop="false" playsinline="true" muted="true">
	      <source src="videos/output_05_compressed204.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
	    </video>
	  </div>
	  <hr>
	  
	  <div class="w3-center">
	    <h2>Abstract</h2>
	  </div>
	  <p>Building 3D perception systems for autonomous vehicles that do not rely on high-density LiDAR is a critical research problem because of the expense of LiDAR systems compared to cameras and other sensors. Recent research has developed a variety of camera-only methods, where features are differentiably "lifted" from the multi-camera images onto the 2D ground plane, yielding a "bird's eye view" (BEV) feature representation of the 3D space around the vehicle. This line of work has produced a variety of novel "lifting" methods, but we observe that other details in the training setups have shifted at the same time, making it unclear <em>what really matters</em> in top-performing methods. We also observe that using cameras alone is not a real-world constraint, considering that additional sensors like <em>radar</em> have been integrated into real vehicles for years already. In this paper, we first of all attempt to elucidate the high-impact factors in the design and training protocol of BEV perception models. We find that batch size and input resolution greatly affect performance, while lifting strategies have a more modest effect&mdash;even a simple parameter-free lifter works well. Second, we demonstrate that radar data can provide a substantial boost to performance, helping to close the gap between camera-only and LiDAR-enabled systems. We analyze the radar usage details that lead to good performance, and invite the community to re-consider this commonly-neglected part of the sensor platform.</p>
	  <hr>
	  <div class="w3-center">
	    <h2>Overview</h2>
	  </div>
	  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
	    <img src="images/car_sensors.png" style="width:40%">
	  </div>
	  <p>
	    To perform driving-related tasks,
	    autonomous vehicles need at least a "bird’s eye view" representation of the 3D space surrounding the vehicle.
	    The challenge is to acquire the bird’s eye view representation from sensors mounted on the vehicle itself,
	    In this work, we present a simple baseline for this perception problem,
	    which is more accurate,
	    faster,
	    and requires fewer parameters,
	    than the current state-of-the-art.
	  </p>
	  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
	    <img src="images/2dtobev_orig.001_cropped.png" style="width:80%">
	  </div>
	  <p>Recent work has focused on innovating new techniques for "lifting" features from the 2D image planes to the BEV plane, delivering the final outputs using cameras alone.</p>
	  <table class="w3-table">
	    <tr>
	      <td style="width:50%;vertical-align:middle">
		<div class="w3-card w3-padding">
		  <img src="images/2dtobev_orig.002_cropped.png" style="width:100%">
		  <p class="w3-small">T. Roddick and R. Cipolla. Predicting semantic map representations from images using pyramid occupancy networks. CVPR 2020</p>
		</div>
	      </td>
	      <td style="width:50%;vertical-align:middle">
		<div class="w3-card w3-padding">
		  <img src="images/2dtobev_orig.003_cropped.png" style="width:100%">
		  <p class="w3-small">J. Philion and S. Fidler. Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3D. ECCV 2020.</p>
		</div>
	      </td>
	    </tr>
	    <tr>
	      <td style="width:50%;vertical-align:middle">
		<div class="w3-card w3-padding">
		  <img src="images/2dtobev_orig.004_cropped.png" style="width:100%">
		  <p class="w3-small">A. Saha, O. M. Maldonado, C. Russell, and R. Bowden. Translating images into maps. ICRA 2022.</p>
		</div>
	      </td>
	      <td style="width:50%;vertical-align:middle">
		<div class="w3-card w3-padding">
		  <img src="images/2dtobev_orig.005_cropped.png" style="width:100%">
		  <p class="w3-small">Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Q. Yu, and J. Dai. Bevformer: Learning bird’seye-view representation from multi-camera images via spatiotemporal transformers. ECCV 2022.</p>
		</div>
	      </td>
	    </tr>
	  </table>

	  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
	    <img src="images/2dtobev_orig.006_cropped.png" style="width:80%">
	  </div>
	  <p>
	    In our proposed baseline model, 
	    we take a parameter-free approach, 
	    which is: we take the 3D coordinate of a voxel,
	    project it into the image feature map, and bilinearly sample there. 
	    Other aspects of our model, in terms of the 2D and BEV CNNs, are similar to related work.
	  </p>

	  <table class="w3-table">
	    <tr>
	      <td style="width:30%;vertical-align:middle">
		<div class="w3-card-black">
		  <div class="w3-padding">
		  <!-- <img src="images/2dtobev_orig.002_cropped.png" style="width:100%"> -->
		  <video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		    <source src="videos/rad_crop.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		  </video>
		  </div>
		</div>
	      </td>
	      <td style="width:70%;vertical-align:middle">
		<div class="w3-padding">
		  <p>
		    As we establish this new simple baseline, we also take the opportunity to question idea of relying on cameras alone, instead of fusing readily-available metric information from, for example, radar.
		    We have seen prior work report that the radar data in nuScenes is perhaps too sparse to be useful.
		  <p>

		  <p>
		    Visualizing the data (on the left), 
		    we can see it is indeed much sparser than LiDAR,
		    but it gives some hints about the metric scene structure, which is very difficult to get from RGB alone.
		  </p>

		</div>
	      </td>
	    </tr>
	  </table>

	  <p>
	    Hypothesizing that <em>some</em> metric information is better than <em>none</em>, 
	    we implement a very simple strategy to fuse the radar information with the RGB feature volume: 
	    we rasterize the radar data into an image matching the BEV dimensions, and simply concatenate it as an additional channel.
	  </p>

	  
	  <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
	    <img src="images/2dtobev_2.001_cropped.png" style="width:80%">
	  </div>
	  <p>Our experiments show that incorporating radar in this way provides a substantial boost in accuracy.</p>
	  
	  
	  <hr>
	  <div class="w3-center">
	    <h2>Results</h2>
	  </div>
	  <p>Please see the paper for quantitative results and analysis. Here, we show video visualizations of the performance.</p>
	  <div class="w3-center">
	    <div class="w3-card">
	      <!-- <div class="w3-padding"><h4>RGB-only model</h4></div> -->
	      <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
		<h4>RGB-only model</h4>

		<!-- <img src="images/bev_vis.png" style="width:100%"> -->
		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/rgb_output_compressed.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
		<p>Top-left: our predictions. Top-right: ground truth. Bottom half: camera input.</p>
	      </div>
	    </div>
	    <br />
	    <div class="w3-card">
	      <!-- <h4>RGB+radar model</h4> -->
	      <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center">
		<h4>RGB+radar model</h4>
		<!-- <img src="images/bev_vis.png" style="width:100%"> -->
		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/rad_output2_small.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
		<p>Top row (in order): LiDAR (for reference), radar (input), our predictions, ground truth. Bottom half: camera input.</p>
	      </div>
	      </div>
	  </div>
	  <p>A clear avenue for future work is to integrate information across time, so that the predictions are less sensitive to momentary occlusions.</p>
	  
	  <hr>

	  <div class="w3-row w3-margin" style="padding-bottom:2em">
	    <div class="w3-center"><h2>Paper</h2></div>
	    <div class="w3-col s0 m1 l2" style="height:10px"></div>
	    <div class="w3-col s6 m3 l2">
	      <a href="https://arxiv.org/pdf/2206.07959"><img class="layered-paper-big" src="images/page1.png" style="width:100%;min-height:200px; margin-right:3em"></a>
	    </div>
	    <div class="w3-col s6 m7 l6" style="padding-left:5em">
	      <div class="cite">
	  	Adam W. Harley, Zhaoyuan Fang, Jie Li, Rares Ambrus, Katerina Fragkiadaki.
	  	<i>Simple-BEV: What Really Matters for Multi-Sensor BEV Perception?</i>
	  	IEEE International Conference on Robotics and Automation (ICRA) 2023.
	      </div>
	      <h3><a href="https://arxiv.org/abs/2206.07959">[abs]</a>&emsp;<a href="https://arxiv.org/pdf/2206.07959.pdf">[pdf]</a>&emsp;<a href="bib.txt">[bibtex]</a></h3>
	      <!-- <h3><a href="https://arxiv.org/abs/2206.07959">[abs]</a>&emsp;<a href="https://arxiv.org/pdf/2206.07959.pdf">[pdf]</a></h3> -->
	    </div>
	    <div class="w3-col s0 m1 l2" style="height:10px"></div>

	  </div>

	  <hr>
	  
	  <div class="w3-row w3-margin" style="padding-bottom:2em">
	    <!-- <h2><a href="bib.txt">[bibtex]</a></h3> -->
	    <div class="w3-center"><h2>Bibtex</h2></div>
	      <div class="w3-code notranslate">
	  	@inproceedings{harley2022simple,<br>
	  	&emsp;author    = {Adam W. Harley and Zhaoyuan Fang and Jie Li and Rares Ambrus and Katerina Fragkiadaki}, <br>
	  	&emsp;title     = {Simple-{BEV}: What Really Matters for Multi-Sensor BEV Perception?},<br>
	  	&emsp;booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},<br>
	  	&emsp;year      = {2023}<br>
	  	}
	      </div>
	    
	  </div>

	  
	  <!-- <hr> -->

	  <!-- <div class="w3-center"><h2>Bibtex</h2></div> -->

	  
	  <!-- end paper container -->


	</div><!-- End Grid -->
      </div><!-- End Page Container -->

  </body>
</html>
